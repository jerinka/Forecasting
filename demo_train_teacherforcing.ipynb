{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link\n",
    "https://romanorac.github.io/machine/learning/2019/09/27/time-series-prediction-with-lstm.html#:~:text=The%20Teacher%20forcing%20is%20a,previous%20output%20as%20current%20input.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for time series prediction\n",
    "\n",
    "The idea of using a Neural Network (NN) to predict the stock price movement on the market is as old as NNs.\n",
    "Intuitively, it seems difficult to predict the future price movement looking only at its past. \n",
    "There are many tutorials on how to predict the price trend or its power, which simplifies the problem.\n",
    "I've decided to try to predict Volume Weighted Average Price with LSTM because it seems challenging and fun.\n",
    "\n",
    "In this blog post, I am going to train a Long Short Term Memory Neural Network (LSTM) with PyTorch on Bitcoin trading data and use the it to predict the price of unseen trading data. \n",
    "I had quite some difficulties with finding intermediate tutorials with a repeatable example of training an LSTM for time series prediction, \n",
    "so I've put together this [Jupyter notebook]({{site.url}}/assets/notebooks/2019-09-27-time-series-prediction-with-lstm.ipynb) to help you to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Necessary Dependencies\n",
    "\n",
    "Let's import the libraries that we are going to use for data manipulation, visualization, training the model, etc.\n",
    "We are going to train the LSTM using PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "from platform import python_version\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version==3.6.9\n",
      "pandas==1.1.5\n",
      "numpy==1.19.5\n",
      "sklearn==0.24.2\n",
      "torch==1.4.0\n",
      "matplotlib==3.3.3\n"
     ]
    }
   ],
   "source": [
    "print(\"python version==%s\" % python_version())\n",
    "print(\"pandas==%s\" % pd.__version__)\n",
    "print(\"numpy==%s\" % np.__version__)\n",
    "print(\"sklearn==%s\" % sklearn.__version__)\n",
    "print(\"torch==%s\" % torch.__version__)\n",
    "print(\"matplotlib==%s\" % matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\n",
    "    \"figure.facecolor\"\n",
    "] = \"w\"  # force white background on plots when using dark mode in JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Data\n",
    "\n",
    "We are going to analyze XBTUSD trading data from BitMex. \n",
    "The daily files are publicly available to [download](https://public.bitmex.com/?prefix=data/trade/).\n",
    "I didn't bother to write the code to download the data automatically,\n",
    "I've simply clicked a couple of times to download the files.\n",
    "\n",
    "Let's lists all the files, read them to a pandas DataFrame and filter the trading data by XBTUSD symbol.\n",
    "It is important to sort the DataFrame by timestamp as there are multiple daily files so that they don't get mixed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/20141122.csv.gz',\n",
       " 'data/20141125.csv.gz',\n",
       " 'data/20141207.csv.gz',\n",
       " 'data/20141222.csv.gz',\n",
       " 'data/20150118.csv.gz',\n",
       " 'data/20150215.csv.gz',\n",
       " 'data/20150312.csv.gz',\n",
       " 'data/20150929.csv.gz',\n",
       " 'data/20210515.csv.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(glob.glob('data/*.csv.gz'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(627983, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(map(pd.read_csv, files))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326126, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.symbol == 'XBTUSD']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.timestamp = pd.to_datetime(df.timestamp.str.replace('D', 'T'))\n",
    "df = df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>side</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>tickDirection</th>\n",
       "      <th>trdMatchID</th>\n",
       "      <th>grossValue</th>\n",
       "      <th>homeNotional</th>\n",
       "      <th>foreignNotional</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-09-29 00:01:16.605839</th>\n",
       "      <td>XBTUSD</td>\n",
       "      <td>Buy</td>\n",
       "      <td>200</td>\n",
       "      <td>240.28</td>\n",
       "      <td>PlusTick</td>\n",
       "      <td>334f5045-8bd5-6aad-30d3-93e7a6591d0d</td>\n",
       "      <td>48056000</td>\n",
       "      <td>0.48056</td>\n",
       "      <td>115.4690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-29 00:01:36.287416</th>\n",
       "      <td>XBTUSD</td>\n",
       "      <td>Buy</td>\n",
       "      <td>200</td>\n",
       "      <td>240.29</td>\n",
       "      <td>PlusTick</td>\n",
       "      <td>1a620bb9-0ceb-fb1f-b1d6-55b6debde7aa</td>\n",
       "      <td>48058000</td>\n",
       "      <td>0.48058</td>\n",
       "      <td>115.4786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-29 00:04:28.309600</th>\n",
       "      <td>XBTUSD</td>\n",
       "      <td>Sell</td>\n",
       "      <td>2500</td>\n",
       "      <td>240.00</td>\n",
       "      <td>MinusTick</td>\n",
       "      <td>00b1f583-7b66-085f-0dd9-b8c4357e1365</td>\n",
       "      <td>600000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>1440.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-29 00:08:33.903957</th>\n",
       "      <td>XBTUSD</td>\n",
       "      <td>Sell</td>\n",
       "      <td>500</td>\n",
       "      <td>240.00</td>\n",
       "      <td>ZeroMinusTick</td>\n",
       "      <td>ac06a0b5-6c2a-c8ea-95cb-7c3236684df9</td>\n",
       "      <td>120000000</td>\n",
       "      <td>1.20000</td>\n",
       "      <td>288.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-29 00:12:15.067759</th>\n",
       "      <td>XBTUSD</td>\n",
       "      <td>Buy</td>\n",
       "      <td>1000</td>\n",
       "      <td>240.50</td>\n",
       "      <td>PlusTick</td>\n",
       "      <td>b5ca42e7-8802-0550-2d32-8124c2fc0db6</td>\n",
       "      <td>240500000</td>\n",
       "      <td>2.40500</td>\n",
       "      <td>578.4025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            symbol  side  size   price  tickDirection  \\\n",
       "timestamp                                                               \n",
       "2015-09-29 00:01:16.605839  XBTUSD   Buy   200  240.28       PlusTick   \n",
       "2015-09-29 00:01:36.287416  XBTUSD   Buy   200  240.29       PlusTick   \n",
       "2015-09-29 00:04:28.309600  XBTUSD  Sell  2500  240.00      MinusTick   \n",
       "2015-09-29 00:08:33.903957  XBTUSD  Sell   500  240.00  ZeroMinusTick   \n",
       "2015-09-29 00:12:15.067759  XBTUSD   Buy  1000  240.50       PlusTick   \n",
       "\n",
       "                                                      trdMatchID  grossValue  \\\n",
       "timestamp                                                                      \n",
       "2015-09-29 00:01:16.605839  334f5045-8bd5-6aad-30d3-93e7a6591d0d    48056000   \n",
       "2015-09-29 00:01:36.287416  1a620bb9-0ceb-fb1f-b1d6-55b6debde7aa    48058000   \n",
       "2015-09-29 00:04:28.309600  00b1f583-7b66-085f-0dd9-b8c4357e1365   600000000   \n",
       "2015-09-29 00:08:33.903957  ac06a0b5-6c2a-c8ea-95cb-7c3236684df9   120000000   \n",
       "2015-09-29 00:12:15.067759  b5ca42e7-8802-0550-2d32-8124c2fc0db6   240500000   \n",
       "\n",
       "                            homeNotional  foreignNotional  \n",
       "timestamp                                                  \n",
       "2015-09-29 00:01:16.605839       0.48056         115.4690  \n",
       "2015-09-29 00:01:36.287416       0.48058         115.4786  \n",
       "2015-09-29 00:04:28.309600       6.00000        1440.0000  \n",
       "2015-09-29 00:08:33.903957       1.20000         288.0000  \n",
       "2015-09-29 00:12:15.067759       2.40500         578.4025  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a trade:\n",
    "- timestamp in microsecond accuracy,\n",
    "- symbol of the contract traded,\n",
    "- side of the trade, buy or sell,\n",
    "- size represents the number of contracts (the number of USD traded),\n",
    "- price of the contract,\n",
    "- tickDirection describes an increase/decrease in the price since the previous transaction,\n",
    "- trdMatchID is the unique trade ID,\n",
    "- grossValue is the number of satoshis exchanged,\n",
    "- homeNotional is the amount of XBT in the trade,\n",
    "- foreignNotional is the amount of USD in the trade.\n",
    "\n",
    "We are going to use 3 columns: timestamp, price and foreignNotional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's calculate [Volume Weighted Average Price (VWAP)](https://www.investopedia.com/terms/v/vwap.asp) in 1 minute time intervals. \n",
    "The data representation where we group trades by the predefined time interval is called time bars. \n",
    "Is this the best way to represent the trade data for modeling? \n",
    "According to Lopez de Prado, trades on the market are not uniformly distributed over time. \n",
    "There are periods with high activity, eg. right before future contracts expire, and grouping of data in predefined time intervals would oversample the data in some time bars and undersample it at others. \n",
    "[Financial Machine Learning Part 0: Bars](https://towardsdatascience.com/financial-machine-learning-part-0-bars-745897d4e4ba) is a nice summary of the 2nd Chapter of Lopez de Prado's book [Advances in Financial Machine Learning Book](https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089).\n",
    "Time bars may not be the best data representation, but we are going to use them regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skycam/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n",
      "/home/skycam/venv3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1ea4e869ca6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m df_vwap = df.groupby(pd.Grouper(freq=\"1Min\")).apply(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeignNotional\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeignNotional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_vwap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \"\"\"\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    184\u001b[0m         ):\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mresult_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mlibreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidApply\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mfast_apply\u001b[0;34m(self, f, sdata, names)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;31m# must return keys::list, values::list, mutated::bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_frame_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.apply_frame_axis0\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1ea4e869ca6a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m df_vwap = df.groupby(pd.Grouper(freq=\"1Min\")).apply(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeignNotional\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeignNotional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_vwap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2242\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  11432\u001b[0m             \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11433\u001b[0m             \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11434\u001b[0;31m             \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11435\u001b[0m         )\n\u001b[1;32m  11436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4247\u001b[0m                 )\n\u001b[1;32m   4248\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4249\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reindex_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnansum\u001b[0;34m(values, axis, skipna, min_count, mask)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m    501\u001b[0m     values, mask, dtype, dtype_max, _ = _get_values(\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     \u001b[0mdtype_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(values, skipna, fill_value, fill_value_typ, mask)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_get_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_maybe_get_mask\u001b[0;34m(values, skipna, mask)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_isna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna\u001b[0;34m(obj, inf_as_na)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlibmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecknull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# hack (for now) because MI registers as ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"isna is not defined for MultiIndex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3.6/lib/python3.6/site-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_typ\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__instancecheck__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_vwap = df.groupby(pd.Grouper(freq=\"1Min\")).apply(\n",
    "    lambda row: pd.np.sum(row.price * row.foreignNotional) / pd.np.sum(row.foreignNotional)\n",
    ")\n",
    "df_vwap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_vwap.plot(figsize=(14, 7))\n",
    "ax.axvline(\"2019-09-01\", linestyle=\"--\", c=\"black\")\n",
    "ax.axvline(\"2019-09-05\", linestyle=\"--\", c=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows time bars with VWAP from 1st of August till the 17th of September 2019.\n",
    "We are going to use the first part of the data for the training set, part in-between for validation set and the last part of the data for the test set (vertical lines are delimiters).\n",
    "We can observe volatility in the VWAP, where the price reaches its highs in the first part of August and lows at the end of August. \n",
    "The high and low are captured in the training set, which is important, as the model most probably wouldn't work well on unseen VWAP intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_vwap[df_vwap.index < \"2019-09-01\"].to_frame(name=\"vwap\")\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_vwap[(df_vwap.index >= \"2019-09-01\") & (df_vwap.index < \"2019-09-05\")].to_frame(name=\"vwap\")\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_vwap[df_vwap.index >= \"2019-09-05\"].to_frame(name='vwap')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling \n",
    "\n",
    "To help the LSTM model to converge faster it is important to scale the data.\n",
    "It is possible that large values in the inputs slows down the learning. \n",
    "We are going to use StandardScaler from sklearn library to scale the data. \n",
    "The scaler is fit on the training set and it is used to transform the unseen trade data on validation and test set. \n",
    "If we would fit the scalar on all data, the model would overfit and it would achieve good results on this data, but performance would suffer on the real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_arr = scaler.fit_transform(df_train)\n",
    "val_arr = scaler.transform(df_val)\n",
    "test_arr = scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling we need to transform the data into a format that is appropriate for modeling with LSTM. \n",
    "We transform the long sequence of data into many shorter sequences (100 time bars per sequence) that are shifted by a single time bar.\n",
    "\n",
    "The plot below shows the first and the second sequence in the training set. \n",
    "The length of both sequences is 100 time bars.\n",
    "We can observe that the target of both sequences is almost the same as the feature, \n",
    "the differences are in the first and in the last time bar.\n",
    "\n",
    "How does the LSTM use the sequence in the training phase?\n",
    "Let's focus on the 1st sequence.\n",
    "The model takes the feature of the time bar at index 0 and it tries to predict the target of the time bar at index 1.\n",
    "Then it takes the feature of the time bar at index 1 and it tries to predict the target of the time bar at index 2, etc.\n",
    "The feature of 2nd sequence is shifted by 1 time bar from the feature of 1st sequence, the feature of 3rd sequence is shifted by 1 time bar from 2nd sequence, etc.\n",
    "With this procedure, we get many shorter sequences that are shifted by a single time bar.\n",
    "\n",
    "Note that in classification or regression tasks, we usually have a set of features and a target that we are trying to predict.\n",
    "In this example with RNN, the feature and the target are from the same sequence, the only difference is that the target is shifted by 1 time bar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(arr, seq_len):\n",
    "    x, y = [], []\n",
    "    for i in range(len(arr) - seq_len):\n",
    "        x_i = arr[i : i + seq_len]\n",
    "        y_i = arr[i + 1 : i + seq_len + 1]\n",
    "        x.append(x_i)\n",
    "        y.append(y_i)\n",
    "    x_arr = np.array(x).reshape(-1, seq_len)\n",
    "    y_arr = np.array(y).reshape(-1, seq_len)\n",
    "    x_var = Variable(torch.from_numpy(x_arr).float())\n",
    "    y_var = Variable(torch.from_numpy(y_arr).float())\n",
    "    return x_var, y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "x_train, y_train = transform_data(train_arr, seq_len)\n",
    "x_val, y_val = transform_data(val_arr, seq_len)\n",
    "x_test, y_test = transform_data(test_arr, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sequence(axes, i, x_train, y_train):\n",
    "    axes[i].set_title(\"%d. Sequence\" % (i + 1))\n",
    "    axes[i].set_xlabel(\"Time bars\")\n",
    "    axes[i].set_ylabel(\"Scaled VWAP\")\n",
    "    axes[i].plot(range(seq_len), x_train[i].cpu().numpy(), color=\"r\", label=\"Feature\")\n",
    "    axes[i].plot(range(1, seq_len + 1), y_train[i].cpu().numpy(), color=\"b\", label=\"Target\")\n",
    "    axes[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 7))\n",
    "plot_sequence(axes, 0, x_train, y_train)\n",
    "plot_sequence(axes, 1, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory Neural Network\n",
    "\n",
    "The Long Short Term Memory neural network is a type of a Recurrent Neural Network (RNN). \n",
    "RNNs use previous time events to inform the later ones.\n",
    "For example, to classify what kind of event is happening in a movie, the model needs to use information about previous events.\n",
    "RNNs work well if the problem requires only recent information to perform the present task. \n",
    "If the problem requires long term dependencies, RNN would struggle to model it.\n",
    "The LSTM was designed to learn long term dependencies. \n",
    "It remembers the information for long periods.\n",
    "LSTM was introduced by [S Hochreiter, J Schmidhuber](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735) in 1997.\n",
    "To learn more about LSTMs read a great [colah blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) which offers a good explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is an implementation of a stateful LSTM for time series prediction. \n",
    "It has an LSTMCell unit and a linear layer to model a sequence of a time series. \n",
    "The model can generate the future values of a time series and \n",
    "it can be trained using teacher forcing (a concept that I am going to describe later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, future=0, y=None):\n",
    "        outputs = []\n",
    "\n",
    "        # reset the state of LSTM\n",
    "        # the state is kept till the end of the sequence\n",
    "        h_t = torch.zeros(input.size(0), self.hidden_size, dtype=torch.float32)\n",
    "        c_t = torch.zeros(input.size(0), self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm(input_t, (h_t, c_t))\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "\n",
    "        for i in range(future):\n",
    "            if y is not None and random.random() > 0.5:\n",
    "                output = y[:, [i]]  # teacher forcing\n",
    "            h_t, c_t = self.lstm(output, (h_t, c_t))\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "class Optimization:\n",
    "    \"\"\" A helper class to train, test and diagnose the LSTM\"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, optimizer, scheduler):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.futures = []\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_batch_data(x, y, batch_size):\n",
    "        for batch, i in enumerate(range(0, len(x) - batch_size, batch_size)):\n",
    "            x_batch = x[i : i + batch_size]\n",
    "            y_batch = y[i : i + batch_size]\n",
    "            yield x_batch, y_batch, batch\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val=None,\n",
    "        y_val=None,\n",
    "        batch_size=100,\n",
    "        n_epochs=15,\n",
    "        do_teacher_forcing=None,\n",
    "    ):\n",
    "        seq_len = x_train.shape[1]\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            self.futures = []\n",
    "\n",
    "            train_loss = 0\n",
    "            for x_batch, y_batch, batch in self.generate_batch_data(x_train, y_train, batch_size):\n",
    "                y_pred = self._predict(x_batch, y_batch, seq_len, do_teacher_forcing)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            self.scheduler.step()\n",
    "            train_loss /= batch\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            self._validation(x_val, y_val, batch_size)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"Epoch %d Train loss: %.2f. Validation loss: %.2f. Avg future: %.2f. Elapsed time: %.2fs.\"\n",
    "                % (epoch + 1, train_loss, self.val_losses[-1], np.average(self.futures), elapsed)\n",
    "            )\n",
    "\n",
    "    def _predict(self, x_batch, y_batch, seq_len, do_teacher_forcing):\n",
    "        if do_teacher_forcing:\n",
    "            future = random.randint(1, int(seq_len) / 2)\n",
    "            limit = x_batch.size(1) - future\n",
    "            y_pred = self.model(x_batch[:, :limit], future=future, y=y_batch[:, limit:])\n",
    "        else:\n",
    "            future = 0\n",
    "            y_pred = self.model(x_batch)\n",
    "        self.futures.append(future)\n",
    "        return y_pred\n",
    "\n",
    "    def _validation(self, x_val, y_val, batch_size):\n",
    "        if x_val is None or y_val is None:\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            batch = 1\n",
    "            for x_batch, y_batch, batch in self.generate_batch_data(x_val, y_val, batch_size):\n",
    "                y_pred = self.model(x_batch)\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= batch\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "    def evaluate(self, x_test, y_test, batch_size, future=1):\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            actual, predicted = [], []\n",
    "            for x_batch, y_batch, batch in self.generate_batch_data(x_test, y_test, batch_size):\n",
    "                y_pred = self.model(x_batch, future=future)\n",
    "                y_pred = (\n",
    "                    y_pred[:, -len(y_batch) :] if y_pred.shape[1] > y_batch.shape[1] else y_pred\n",
    "                )\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                test_loss += loss.item()\n",
    "                actual += torch.squeeze(y_batch[:, -1]).data.cpu().numpy().tolist()\n",
    "                predicted += torch.squeeze(y_pred[:, -1]).data.cpu().numpy().tolist()\n",
    "            test_loss /= batch\n",
    "            return actual, predicted, test_loss\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(scaler, model, x_sample, future=1000):\n",
    "    \"\"\" Generate future values for x_sample with the model \"\"\"\n",
    "    y_pred_tensor = model(x_sample, future=future)\n",
    "    y_pred = y_pred_tensor.cpu().tolist()\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(actual, predicted):\n",
    "    return pd.DataFrame({\"actual\": actual, \"predicted\": predicted})\n",
    "\n",
    "\n",
    "def inverse_transform(scalar, df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = scaler.inverse_transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM\n",
    "\n",
    "We train LSTM with 21 hidden units. \n",
    "A lower number of units is used so that it is less likely that LSTM would perfectly memorize the sequence.\n",
    "We use Mean Square Error loss function and Adam optimizer. \n",
    "Learning rate is set to 0.001 and it decays every 5 epochs. \n",
    "We train the model with 100 sequences per batch for 15 epochs.\n",
    "From the plot below, we can observe that training and validation loss converge after sixth epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Model(input_size=1, hidden_size=21, output_size=1)\n",
    "loss_fn_1 = nn.MSELoss()\n",
    "optimizer_1 = optim.Adam(model_1.parameters(), lr=1e-3)\n",
    "scheduler_1 = optim.lr_scheduler.StepLR(optimizer_1, step_size=5, gamma=0.1)\n",
    "optimization_1 = Optimization(model_1, loss_fn_1, optimizer_1, scheduler_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_1.train(x_train, y_train, x_val, y_val, do_teacher_forcing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_1.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model on the test set.\n",
    "The future parameter is set to 5, which means that the model outputs the VWAP where it believes it will be in the next 5 time bars (5 minutes in our example).\n",
    "This should make the price change visible few time bars before it occurs.\n",
    "\n",
    "On the plot below, we can observe that predicted values closely match actual values of VWAP, which seems great on the first sight.\n",
    "But the future parameter was set to 5, which means that the orange line should react before a spike occurs instead of covering it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_1, predicted_1, test_loss_1 = optimization_1.evaluate(x_test, y_test, future=5, batch_size=100)\n",
    "df_result_1 = to_dataframe(actual_1, predicted_1) \n",
    "df_result_1 = inverse_transform(scaler, df_result_1, ['actual', 'predicted'])\n",
    "df_result_1.plot(figsize=(14, 7))\n",
    "print(\"Test loss %.4f\" % test_loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we zoom into the spikes (one on the start and the other on the end of the time series).\n",
    "We can observe that predicted values mimic the actual values.\n",
    "When the actual value changes direction, predicted value follows, which doesn't help us much.\n",
    "The same happens when we increase the future parameter (like it doesn't affect the predicted line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 7))\n",
    "df_result_1.iloc[2350:2450].plot(ax=axes[0], figsize=(14, 7))\n",
    "df_result_1.iloc[16000:17500].plot(ax=axes[1], figsize=(14, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 1000 time bars for the first test sequence with the model and compare predicted, generated and actual VWAP.\n",
    "We can observe that while the model outputs predicted values, they are close to actual values.\n",
    "But when it starts to generate values, the output almost resembles the sine wave. \n",
    "After certain period values converge to 9600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = x_test[0].reshape(1, -1)\n",
    "y_sample = df_test.vwap[:1100]\n",
    " \n",
    "y_pred1 = generate_sequence(scaler, optimization_1.model, x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(range(100), y_pred1[0][:100], color=\"blue\", lw=2, label=\"Predicted VWAP\")\n",
    "plt.plot(range(100, 1100), y_pred1[0][100:], \"--\", color=\"blue\", lw=2, label=\"Generated VWAP\")\n",
    "plt.plot(range(0, 1100), y_sample, color=\"red\", label=\"Actual VWAP\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behavior could occur because the model was trained only with true inputs and never with generated inputs.\n",
    "When the model gets fed the generated output on the input, it does a poor job of generating the next values.\n",
    "Teacher forcing is a concept that deals with this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Forcing\n",
    "\n",
    "The [Teacher forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/) is a method for training Recurrent Neural Networks that use the output from a previous time step as an input.\n",
    "When the RNN is trained, it can generate a sequence by using the previous output as current input.\n",
    "The same process can be used during training, but the model can become unstable or it does not converge.\n",
    "Teacher forcing is an approach to address those issues during training. \n",
    "It is commonly used in language models. \n",
    "\n",
    "We are going to use an extension of Teacher forcing, called [Scheduled sampling](https://arxiv.org/abs/1506.03099).\n",
    "The model will use its generated output as an input with a certain probability during training.\n",
    "At first, the probability of a model seeing its generated output is small and then it gradually increases during training.\n",
    "Note that in this example, we use a random probability, which doesn't increase during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model with the same parameters as before but with the teacher forcing enabled.\n",
    "After 7 epochs, the training and validation loss converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model(input_size=1, hidden_size=21, output_size=1)\n",
    "loss_fn_2 = nn.MSELoss()\n",
    "optimizer_2 = optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "scheduler_2 = optim.lr_scheduler.StepLR(optimizer_2, step_size=5, gamma=0.1)\n",
    "optimization_2 = Optimization(model_2, loss_fn_2,  optimizer_2, scheduler_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_2.train(x_train, y_train, x_val, y_val, do_teacher_forcing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_2.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a similar predicted sequence as before.\n",
    "When we zoom into the spikes, similar behavior of the model can be observed, where predicted values mimic the actual values.\n",
    "It seems like teacher forcing didn't solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_2, predicted_2, test_loss_2 = optimization_2.evaluate(x_test, y_test, batch_size=100, future=5)\n",
    "df_result_2 = to_dataframe(actual_2, predicted_2)\n",
    "df_result_2 = inverse_transform(scaler, df_result_2, [\"actual\", \"predicted\"])\n",
    "df_result_2.plot(figsize=(14, 7))\n",
    "print(\"Test loss %.4f\" % test_loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 7))\n",
    "df_result_2.iloc[2350:2450].plot(ax=axes[0], figsize=(14, 7))\n",
    "df_result_2.iloc[16000:17500].plot(ax=axes[1], figsize=(14, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 1000 time bars for the first test sequence with the model trained with teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = generate_sequence(scaler, optimization_2.model, x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(range(100), y_pred2[0][:100], color=\"blue\", lw=2, label=\"Predicted VWAP\")\n",
    "plt.plot(range(100, 1100), y_pred2[0][100:], \"--\", color=\"blue\", lw=2, label=\"Generated VWAP\")\n",
    "plt.plot(range(0, 1100), y_sample, color=\"red\", label=\"Actual VWAP\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting observation about the generated sequence is that the generated values from the model trained with teacher forcing need longer to converge.\n",
    "Another observation about the generated sequence is that when it is increasing, it will continue to increase to some point, \n",
    "then start to decrease and the pattern repeats until the sequence converges.\n",
    "The pattern looks like a sine wave with decreasing amplitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The result of this experiment is that the predictions of the model mimic actual values of the sequence.\n",
    "The first and second model do not detect price changes before they occur.\n",
    "Adding another feature (like volume) might help the model to detect the price changes before they occur, but then the model would need to generate two features to use the output of those as an input in next step, which would complicate the model.\n",
    "Using a more complex model (multiple LSTMCells, increase the number of hidden units) might not help as the model has the capacity to predict VWAP time series as seen in the plots above.\n",
    "More advanced methods of teacher forcing might help so that the model would improve sequence generation skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    " - [Time Sequence Prediction](https://github.com/pytorch/examples/tree/master/time_sequence_prediction)\n",
    " - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    " - [What is Teacher Forcing for Recurrent Neural Networks?](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)\n",
    " - [Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks](https://arxiv.org/abs/1506.03099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
